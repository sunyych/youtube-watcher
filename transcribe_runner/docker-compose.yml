# Run this on the GPU machine. Point queue's TRANSCRIBE_RUNNER_URL to this host:port.
# Uses Dockerfile.gpu (CUDA 12) so WHISPER_DEVICE=cuda works.
services:
  transcribe_runner:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: transcribe_runner
    ports:
      - "${PORT:-8765}:8765"
    environment:
      PORT: 8765
      WHISPER_MODEL_SIZE: ${WHISPER_MODEL_SIZE:-medium}
      WHISPER_DEVICE: ${WHISPER_DEVICE:-cuda}
      MAX_CONCURRENT_JOBS: ${MAX_CONCURRENT_JOBS:-3}
      NUM_GPUS: ${NUM_GPUS:-3}
      AUDIO_TARGET_SAMPLE_RATE: ${AUDIO_TARGET_SAMPLE_RATE:-16000}
      VAD_THRESHOLD: ${VAD_THRESHOLD:-0.5}
      VAD_MIN_SILENCE_DURATION_MS: ${VAD_MIN_SILENCE_DURATION_MS:-2000}
      VAD_SPEECH_PAD_MS: ${VAD_SPEECH_PAD_MS:-400}
      VAD_MAX_SPEECH_DURATION_S: ${VAD_MAX_SPEECH_DURATION_S:-30}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
